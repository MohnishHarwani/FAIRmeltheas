{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92face96",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad365cd",
   "metadata": {},
   "source": [
    "Importing the libraries necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521eee90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:04.633596Z",
     "start_time": "2024-09-24T22:36:58.669198Z"
    }
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from simtool import findInstalledSimToolNotebooks, searchForSimTool\n",
    "from simtool import getSimToolInputs, getSimToolOutputs, Run\n",
    "MeltHEA = searchForSimTool('meltheas')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import widgets, HBox\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.manifold import TSNE, MDS, LocallyLinearEmbedding, Isomap, SpectralEmbedding\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import nanohubremote as nr\n",
    "from scipy.stats import norm\n",
    "\n",
    "from lolopy.learners import RandomForestRegressor\n",
    "from lolopy.metrics import root_mean_squared_error\n",
    "from lolopy.metrics import standard_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9d2182",
   "metadata": {},
   "source": [
    "# Querying Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde03f49",
   "metadata": {},
   "source": [
    "Interacting with the nanoHub API to query relevant data from the MELTHEAS simtool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ece43c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:12.030101Z",
     "start_time": "2024-09-24T22:37:04.660272Z"
    }
   },
   "outputs": [],
   "source": [
    "# CREDENTIALS\n",
    "auth_data = { 'grant_type' : 'tool' }\n",
    "with open(os.environ[\"SESSIONDIR\"]+\"/resources\") as file:\n",
    "    lines = [line.split(\" \", 1) for line in file.readlines()]\n",
    "    properties = {line[0].strip(): line[1].strip() for line in lines if len(line)==2}\n",
    "    auth_data[\"sessiontoken\"] = properties[\"session_token\"]\n",
    "    auth_data[\"sessionnum\"] = properties[\"sessionid\"]\n",
    "    \n",
    "session = nr.Tools(auth_data)\n",
    "\n",
    "# TOOL NAME\n",
    "tool = 'meltheas' \n",
    "\n",
    "# QUERY FOR INPUTS AND OUTPUTS\n",
    "req_json = session.requestPost('results/dbexplorer/tool_detail?simtool=true', data={'tool': tool})\n",
    "req_json = req_json.json()\n",
    "parameters = req_json['results']\n",
    "inputs = list(parameters[0][tool]['input'].keys())\n",
    "outputs = list(parameters[0][tool]['output'].keys())\n",
    "\n",
    "\n",
    "# QUERY EVERYTHING FROM THE DATABASE\n",
    "search = {\n",
    "    'tool':tool,\n",
    "    'filters':json.dumps([\n",
    "        {'field':'input.Tsolid','operation':'>=','value':'2'},     # Filters included to query [Everything] in the cached database\n",
    "        {'field':'input.composition1','operation':'<=','value':'0.5'},\n",
    "        {'field':'input.composition2','operation':'<=','value':'0.5'},\n",
    "        {'field':'input.composition3','operation':'<=','value':'0.5'},\n",
    "        {'field':'input.composition4','operation':'<=','value':'0.5'},\n",
    "        {'field':'input.composition5','operation':'<=','value':'0.5'},        \n",
    "    ]),\n",
    "    'results':json.dumps(['input.composition1','input.composition2','input.composition3','input.composition4','input.composition5',\n",
    "                          'input.Tsolid', 'input.Tliquid',\n",
    "                          'output.Coexistence', 'output.Converged', 'output.melting_temperature',\n",
    "                          'output.melting_temperature_ci', 'output.fraction_solid', 'output.fraction_liquid', 'output.counts_array',]),    # Selected Parameters (Inputs/Outputs) that will be requested from the query\n",
    "    'limit':0,    \n",
    "    'revision':0,\n",
    " }\n",
    "\n",
    "req_json = session.requestPost('results/dbexplorer/search?simtool=true', data=search, timeout=30)\n",
    "results = req_json.json()\n",
    "\n",
    "#RESET INDEX AND DROP NA VALUES\n",
    "complete_dataframe = pd.DataFrame(results['results']).dropna()\n",
    "complete_dataframe = complete_dataframe.reset_index(drop=True)\n",
    "complete_dataframe_copy = complete_dataframe.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ec9e3",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a605af5",
   "metadata": {},
   "source": [
    "Preprocess the data to include composition-derived features (Rule-of-Mixtures) as outlined in [Farache et. al](https://nanohub.org/resources/activemeltheas). Filter into known sets and unknown sets of data based on alloys discovered in [Farache et. al](https://nanohub.org/resources/activemeltheas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22e8dc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:33.128045Z",
     "start_time": "2024-09-24T22:37:30.691853Z"
    }
   },
   "outputs": [],
   "source": [
    "#RENAME COLUMNS BASED ON ELEMENT NAME\n",
    "columns = {'input.composition1': 'Cr', 'input.composition2': 'Co', 'input.composition3': 'Cu', 'input.composition4': 'Fe', 'input.composition5': 'Ni'}\n",
    "complete_dataframe.rename(columns = columns, inplace = True)\n",
    "\n",
    "#USE ONLY DATA WITH CONVERGED AND COEXISTED SIMULATIONS\n",
    "complete_dataframe = complete_dataframe.loc[(complete_dataframe['output.Coexistence'] == 1) & (complete_dataframe['output.Converged'] == 1)]\n",
    "\n",
    "#DROP DUPLICATES, CONTAINING UNIQUE VALUES FOR EACH COMPOSITION\n",
    "complete_dataframe = complete_dataframe.drop_duplicates(subset=['Cr', 'Co', 'Cu', 'Fe', 'Ni'])\n",
    "\n",
    "#LIST OF DATES FOR EACH COMPOSITION RAN\n",
    "df = pd.read_csv('cleaned_list.txt', sep=' ', header=None)\n",
    "df[8] = [x.replace(\"_\", \"/\") for x in df[8]]\n",
    "#display(df)\n",
    "runs_file = complete_dataframe.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35481675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:33.977068Z",
     "start_time": "2024-09-24T22:37:33.783528Z"
    }
   },
   "outputs": [],
   "source": [
    "#ADD MONTH, DAY, YEAR TO EACH VALUE\n",
    "comparison = runs_file[runs_file['squid'].isin(df[8])]\n",
    "df = df.rename({8 : 'squid'}, axis = 1)\n",
    "merged_df = pd.merge(comparison, df[[5, 6, 7, 'squid']], left_on='squid', right_on='squid', how='left')\n",
    "\n",
    "# Drop the 'squid' column from the merged DataFrame as it's no longer needed\n",
    "merged_df.drop('squid', axis=1, inplace=True)\n",
    "merged_df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "# Rename the columns 5, 6, 7, to month, day, year\n",
    "merged_df.rename(columns={5: 'Month', 6: 'Day', 7: 'Year'}, inplace=True)\n",
    "\n",
    "#Drop duplicates\n",
    "merged_df = merged_df.drop_duplicates(subset=['Cr', 'Co', 'Cu', 'Fe', 'Ni'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271733fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:34.294124Z",
     "start_time": "2024-09-24T22:37:34.289096Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_dates(df, month, day, year):\n",
    "    target_date = datetime(year, month, day)\n",
    "    df['Date'] = pd.to_datetime(df[['Year', 'Month', 'Day']])\n",
    "    filtered_df = df[df['Date'] <= target_date]\n",
    "    return filtered_df\n",
    "def convert_abbr_to_num(df, column_name):\n",
    "    month_mapping = {'Jan':  1, 'Feb':  2, 'Mar':  3, 'Apr':  4, 'May':  5, 'Jun':  6,\n",
    "                     'Jul':  7, 'Aug':  8, 'Sep':  9, 'Oct':  10, 'Nov':  11, 'Dec':  12}\n",
    "    df[column_name] = df[column_name].apply(lambda x: month_mapping.get(x, x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8589535d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:34.743874Z",
     "start_time": "2024-09-24T22:37:34.731862Z"
    }
   },
   "outputs": [],
   "source": [
    "#RENAME AND FILTER BY DATE\n",
    "prior_to_2022_data = convert_abbr_to_num(merged_df, 'Month')\n",
    "prior_to_2022_data = filter_dates(prior_to_2022_data, 1, 1, 2022)\n",
    "prior_to_2022_data = prior_to_2022_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07ff3b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:35.176130Z",
     "start_time": "2024-09-24T22:37:35.172152Z"
    }
   },
   "outputs": [],
   "source": [
    "#SUBSET RELEVANT COLUMNS\n",
    "columns = ['Cr', 'Co', 'Cu', 'Fe', 'Ni', 'input.Tsolid', 'input.Tliquid', 'output.melting_temperature']\n",
    "known = prior_to_2022_data[columns].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686df554",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:35.644477Z",
     "start_time": "2024-09-24T22:37:35.601750Z"
    }
   },
   "outputs": [],
   "source": [
    "display(known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24d8cfc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:36.114818Z",
     "start_time": "2024-09-24T22:37:36.109662Z"
    }
   },
   "outputs": [],
   "source": [
    "known.drop(columns = {'input.Tsolid', 'input.Tliquid'}, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e6f38e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:37.671461Z",
     "start_time": "2024-09-24T22:37:37.518005Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create positions with 0.1 step size\n",
    "nx = 11\n",
    "x1 = np.linspace(0, 1, nx)\n",
    "\n",
    "# Generate combinations directly using itertools\n",
    "combinations = product(x1, repeat=5)\n",
    "\n",
    "# Convert to DataFrame and filter\n",
    "columns = ['Cr', 'Co', 'Cu', 'Fe', 'Ni']\n",
    "plausible_compositions = pd.DataFrame(combinations, columns=columns)\n",
    "\n",
    "# Check if the sum is exactly equal to 1\n",
    "plausible_compositions['sum'] = plausible_compositions.sum(axis=1)\n",
    "plausible_compositions = plausible_compositions[plausible_compositions['sum'].eq(1)]\n",
    "\n",
    "# Filter values <= 0.5\n",
    "for col in columns:\n",
    "    plausible_compositions = plausible_compositions[plausible_compositions[col] <= 0.5]\n",
    "\n",
    "# Round values to 0.1\n",
    "plausible_compositions = plausible_compositions.round(1)\n",
    "\n",
    "# Drop the 'sum' column\n",
    "plausible_compositions = plausible_compositions.drop('sum', axis=1)\n",
    "\n",
    "# Show the DataFrame\n",
    "unknown = plausible_compositions\n",
    "unknown.reset_index(inplace=True)\n",
    "unknown.drop('index', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2e7742",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:38.253191Z",
     "start_time": "2024-09-24T22:37:38.224571Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = unknown.copy()\n",
    "df2 = known.copy()\n",
    "df1.round(1)\n",
    "df2.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2640b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:40.692178Z",
     "start_time": "2024-09-24T22:37:40.659736Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merge df1 and df2 on the first 5 columns\n",
    "merged_df = pd.merge(df1, df2, on=['Cr', 'Co', 'Cu', 'Fe', 'Ni'], how='left', indicator=True)\n",
    "\n",
    "# Keep only the rows from df1 that don't have duplicates in the merged dataframe\n",
    "filtered_df1 = merged_df[merged_df['_merge'] == 'left_only'].drop(columns='_merge')\n",
    "\n",
    "unknown = filtered_df1\n",
    "unknown.drop('output.melting_temperature', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3bc8d8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:41.622504Z",
     "start_time": "2024-09-24T22:37:41.619403Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = ['Cr', 'Co', 'Cu', 'Fe', 'Ni']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3de45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:44.263197Z",
     "start_time": "2024-09-24T22:37:44.252847Z"
    }
   },
   "outputs": [],
   "source": [
    "#RULE OF MIXTURES FEATURE CALCULATION\n",
    "\n",
    "data = {\n",
    "  \"Cr\": [2.790000e+02, 1.400000e+00, 1.270000e-07, 4.900000e-06, 1.120000e+03, 2.944000e+03, 5.199610e+01, 2.100000e-01, 7.140000e+03, 1.660000e+00, 2.180000e+03],\n",
    "  \"Co\": [2.090000e+02, 1.350000e+00, 6.000000e-08, 1.300000e-05, 7.000000e+02, 3.200000e+03, 5.893319e+01, 3.100000e-01, 8.900000e+03, 1.880000e+00, 1.768000e+03],\n",
    "  \"Cu\": [1.300000e+02, 1.350000e+00, 1.720000e-08, 1.650000e-05, 8.740000e+02, 3.200000e+03, 6.354600e+01, 3.400000e-01, 8.920000e+03, 1.900000e+00, 1.357770e+03],\n",
    "  \"Fe\": [2.110000e+02, 1.400000e+00, 1.000000e-07, 1.180000e-05, 4.900000e+02, 3.134000e+03, 5.584500e+01, 2.900000e-01, 7.874000e+03, 1.830000e+00, 1.811000e+03],\n",
    "  \"Ni\": [2.000000e+02, 1.350000e+00, 7.200000e-08, 1.340000e-05, 7.000000e+02, 3.186000e+03, 5.869340e+01, 3.100000e-01, 8.908000e+03, 1.910000e+00, 1.728000e+03]\n",
    "}\n",
    "\n",
    "index = ['youngs_modulus', 'atomic_radius', 'electrical_resistivity', 'CTE', 'hardness', 'boiling_point', 'atomic_mass', 'poissons_ratio', 'density_of_solid', 'en_gosh', 'melting_point']\n",
    "\n",
    "c_df = pd.DataFrame(data, index=index)\n",
    "\n",
    "actual_qued_values = ['youngs_modulus', 'atomic_radius', 'electrical_resistivity','CTE','hardness','boiling_point',\n",
    "                         'atomic_mass','poissons_ratio','density_of_solid','en_gosh','melting_point']\n",
    "\n",
    "\n",
    "#Apply Rule of Mixtures\n",
    "def rule_mixtures(df2,df,sample): \n",
    "    #Creats column of first rule mixture descriptors to concatenate to\n",
    "    cr = 0\n",
    "    for comps in sample:\n",
    "        cr = cr+df2[comps]*df[comps][0]\n",
    "    cr = cr.to_frame()\n",
    "\n",
    "    #Concatenate rest of compositions with rule of mixtures applied\n",
    "    for x in range(1,len(actual_qued_values)):\n",
    "        cf = 0\n",
    "        for comps in sample:\n",
    "            cf = cf + df2[comps]*df[comps][x]\n",
    "        cf.to_frame()\n",
    "        cr = pd.concat([cr,cf], axis=1)\n",
    "\n",
    "    #Renames columns to descriptors\n",
    "    cr.columns = actual_qued_values\n",
    "    return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b40a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:45.489054Z",
     "start_time": "2024-09-24T22:37:45.390503Z"
    }
   },
   "outputs": [],
   "source": [
    "#ADD FEATURES TO BOTH SETS\n",
    "uk = rule_mixtures(unknown, c_df, sample)\n",
    "unknown = pd.concat([unknown, uk], axis=1)\n",
    "\n",
    "kn = rule_mixtures(known, c_df, sample)\n",
    "known = pd.concat([known, kn], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c5a35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:46.025586Z",
     "start_time": "2024-09-24T22:37:46.020557Z"
    }
   },
   "outputs": [],
   "source": [
    "#REARRANGE COLUMNS\n",
    "column_to_move = known.pop('output.melting_temperature')\n",
    "known['output.melting_temperature'] = column_to_move\n",
    "unknown['Predicted Melting Temp'] = 0\n",
    "unknown.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949782ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:46.522168Z",
     "start_time": "2024-09-24T22:37:46.519646Z"
    }
   },
   "outputs": [],
   "source": [
    "#display(known)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fbcf59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:47.000188Z",
     "start_time": "2024-09-24T22:37:46.997544Z"
    }
   },
   "outputs": [],
   "source": [
    "#display(unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fde813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0736b615",
   "metadata": {},
   "source": [
    "# Active Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405d9a5",
   "metadata": {},
   "source": [
    "Active Learning Loop to filter for alloys with the lowest melting temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badee98e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:53.791379Z",
     "start_time": "2024-09-24T22:37:53.769779Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = getSimToolInputs(MeltHEA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98127210",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:37:54.886185Z",
     "start_time": "2024-09-24T22:37:54.879980Z"
    }
   },
   "outputs": [],
   "source": [
    "#RELEVANT DATA TO SAVE\n",
    "list_df = pd.DataFrame(columns = ['exp', 'Cr', 'Co', 'Cu', 'Fe', 'Ni', 'melting_temp', 'ci', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f3d5c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T22:40:31.492179Z",
     "start_time": "2024-09-24T22:40:31.485448Z"
    }
   },
   "outputs": [],
   "source": [
    "#SET INPUTS\n",
    "n_steps = 15 # number of expirements\n",
    "time_per_simmul = 100 #Time per simulation\n",
    "box_len = 18 #box length\n",
    "iaf = \"MM\" #select acquisition function (MM,UCB,MLI,MEI,MU,Random)\n",
    "\n",
    "#SPECIFY K FOR UCB\n",
    "k = 1\n",
    "\n",
    "#PARAMETERS\n",
    "known_min = 0\n",
    "counter_list = []\n",
    "not_known = unknown.copy()\n",
    "patience_counter = 0\n",
    "not_known = unknown.copy()\n",
    "counter = 0\n",
    "\n",
    "#PIECEWISE FUNCTION FOR MEI ACQUISTION FUNCTION\n",
    "def piecewise(m_row, s_row):\n",
    "    outputs = []\n",
    "    for m, s in zip(m_row, s_row):\n",
    "        if s > 0:\n",
    "            output = s * norm.pdf(m / s) + m * norm.cdf(m / s)\n",
    "        else:\n",
    "            output = max(np.max(m), 0)\n",
    "        outputs.append(output)\n",
    "    return outputs\n",
    "\n",
    "tsol_slope = 0.6026\n",
    "tliq_slope = 1.2052\n",
    "tsol_intercept = 98.09\n",
    "tliq_intercept = 196.18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bd6267",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-09-24T22:40:33.363Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(n_steps):\n",
    "\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    print(\"Experiment # \", i)\n",
    "    \n",
    "    #Random Forest shit\n",
    "    \n",
    "    X_train = known.iloc[:, :16].values # Features\n",
    "    y_train = known.iloc[:, -1].values    # Target\n",
    "\n",
    "    # Train the Random Forest Model using lolopy\n",
    "    rf = RandomForestRegressor(num_trees = 350)\n",
    "    rf.fit(X_train, y_train)\n",
    "\n",
    "    # Prepare the unknown data\n",
    "    X_unknown = not_known.iloc[:, :16] # Features from unknown DataFrame\n",
    "\n",
    "    # Predict on the unknown data and get the standard deviation of the predictions\n",
    "    y_pred, y_std = rf.predict(X_unknown, return_std=True)\n",
    "\n",
    "    # Convert the predictions and standard deviations to Numpy arrays\n",
    "    predictions_np = np.array(y_pred).reshape(-1, 1)\n",
    "    std_dev_array = np.array(y_std).reshape(-1, 1)\n",
    "\n",
    "\n",
    "\n",
    "    # Append the predictions as a new column to the DataFrame\n",
    "    not_known['Predicted Melting Temp'] = predictions_np\n",
    "    \n",
    "    not_known.reset_index(inplace = True, drop = True)\n",
    "\n",
    "    \n",
    "    # EARLY STOPPING / ACQUISITION - need to add rest of the acquisition functions back later\n",
    "    \n",
    "    if iaf == \"MM\":\n",
    "        min_prediction_index = not_known['Predicted Melting Temp'].idxmin()\n",
    "        prediction_row = not_known.iloc[min_prediction_index, :]\n",
    "        prediction_row = pd.DataFrame(prediction_row).transpose()\n",
    "        \n",
    "        print(\"Min Prediction Row Selected\", prediction_row)\n",
    "        \n",
    "        not_known = not_known.drop(min_prediction_index)\n",
    "        not_known = not_known.reset_index(drop = True)\n",
    "        \n",
    "        min_prediction_value = prediction_row['Predicted Melting Temp'].item()\n",
    "        print('Min Prediction Value', min_prediction_value)\n",
    "\n",
    "        if min_prediction_value >= known_min:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "            known_min = min_prediction_value\n",
    "    elif iaf == \"UCB\":\n",
    "        \n",
    "        expression_array = predictions_np + k * std_dev_array\n",
    "        expression_array = expression_array * (-1)\n",
    "        \n",
    "        min_prediction_index = np.argmax(expression_array)\n",
    "        \n",
    "        prediction_row = not_known.iloc[min_prediction_index, :]\n",
    "        prediction_row = pd.DataFrame(prediction_row).transpose()\n",
    "        \n",
    "        not_known = not_known.drop(min_prediction_index)\n",
    "        not_known = not_known.reset_index(drop = True)\n",
    "        \n",
    "        min_prediction_value = prediction_row['Predicted Melting Temp'].item()\n",
    "        print('Min Prediction Value', min_prediction_value)\n",
    "        \n",
    "        if min_prediction_value >= known_min:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "            known_min = min_prediction_value\n",
    "        \n",
    "    elif iaf == \"MU\":\n",
    "        min_prediction_index = np.argmax(std_dev_array)\n",
    "        \n",
    "        prediction_row = not_known.iloc[min_prediction_index, :]\n",
    "        prediction_row = pd.DataFrame(prediction_row).transpose()\n",
    "        \n",
    "        not_known = not_known.drop(min_prediction_index)\n",
    "        not_known = not_known.reset_index(drop = True)\n",
    "        \n",
    "        min_prediction_value = prediction_row['Predicted Melting Temp'].item()\n",
    "        print('Min Prediction Value', min_prediction_value)\n",
    "        \n",
    "        if min_prediction_value >= known_min:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "            known_min = min_prediction_value\n",
    "    elif iaf == \"MLI\":\n",
    "        max_pred = np.max(predictions_np)\n",
    "        \n",
    "        expression_array = max_pred - predictions_np\n",
    "        expression_array = expression_array/std_dev_array\n",
    "        min_prediction_index = np.argmax(expression_array)\n",
    "\n",
    "        prediction_row = not_known.iloc[min_prediction_index, :]\n",
    "        prediction_row = pd.DataFrame(prediction_row).transpose()\n",
    "        \n",
    "        print(prediction_row)\n",
    "        \n",
    "        not_known = not_known.drop(min_prediction_index)\n",
    "        not_known = not_known.reset_index(drop = True)\n",
    "        \n",
    "        min_prediction_value = prediction_row['Predicted Melting Temp'].item()\n",
    "        print('Min Prediction Value', min_prediction_value)  \n",
    "        \n",
    "        if min_prediction_value >= known_min:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "            known_min = min_prediction_value\n",
    "    elif iaf == \"MEI\":\n",
    "        max_pred = np.max(predictions_np)\n",
    "        \n",
    "        expression_array = predictions_np - max_pred\n",
    "        expression_array = piecewise(expression_array, std_dev_array)\n",
    "        expression_array = [-x for x in expression_array]\n",
    "        \n",
    "        min_prediction_index = np.argmax(expression_array)\n",
    "        \n",
    "        \n",
    "        prediction_row = not_known.iloc[min_prediction_index, :]\n",
    "        prediction_row = pd.DataFrame(prediction_row).transpose()\n",
    "        \n",
    "        print(prediction_row)\n",
    "        \n",
    "        not_known = not_known.drop(min_prediction_index)\n",
    "        not_known = not_known.reset_index(drop = True)\n",
    "        \n",
    "        min_prediction_value = prediction_row['Predicted Melting Temp'].item()\n",
    "        print('Min Prediction Value', min_prediction_value)  \n",
    "        \n",
    "        if min_prediction_value >= known_min:\n",
    "            patience_counter +=1\n",
    "        else:\n",
    "            patience_counter = 0\n",
    "            known_min = min_prediction_value\n",
    "    else:\n",
    "        prediction_row = not_known.sample(n=1) \n",
    "        \n",
    "\n",
    "    \n",
    "    if patience_counter > 15:\n",
    "        print(\"Early Stop\")\n",
    "        break\n",
    "        \n",
    "    \n",
    "#Running Sim - Any writes need to be added\n",
    "\n",
    "\n",
    "    inputs.time.value = 100 # -> number of picoseconds to run\n",
    "    inputs.composition1.value = float(prediction_row['Cr'])\n",
    "    inputs.composition2.value = float(prediction_row['Co'])\n",
    "    inputs.composition3.value = float(prediction_row['Cu'])\n",
    "    inputs.composition4.value = float(prediction_row['Fe'])\n",
    "    inputs.composition5.value = float(prediction_row['Ni'])\n",
    "    inputs.box_length.value = box_len\n",
    "\n",
    "    \n",
    "    prediction = min_prediction_value\n",
    "    \n",
    "    inputs.Tsolid.value = (tsol_slope * prediction) + tsol_intercept\n",
    "    inputs.Tliquid.value = (tliq_slope * prediction) + tliq_intercept\n",
    "    r = Run(MeltHEA, inputs)\n",
    "    \n",
    "    coexist = bool(r.read('Coexistence'))\n",
    "    conv = bool(r.read('Converged'))\n",
    "    comp = 2\n",
    "    counter = 1\n",
    "    while ((coexist and conv) is False):\n",
    "        counter += 1\n",
    "        \n",
    "        inputs.time.value = 100 # -> number of picoseconds to run\n",
    "        inputs.composition1.value = prediction_row.iloc[0, 0]\n",
    "        inputs.composition2.value = prediction_row.iloc[0, 1]\n",
    "        inputs.composition3.value = prediction_row.iloc[0, 2]\n",
    "        inputs.composition4.value = prediction_row.iloc[0, 3]\n",
    "        inputs.composition5.value = prediction_row.iloc[0, 4]\n",
    "        inputs.box_length.value = 18\n",
    "\n",
    "        \n",
    "        if ((coexist) and (not conv)):\n",
    "            inputs.time.value = inputs.time.value + 50\n",
    "        else:\n",
    "            if (float(r.read('fraction_solid')) >= float(r.read('fraction_liquid'))):\n",
    "                prediction = prediction * 1.025\n",
    "            else:\n",
    "                prediction = prediction * 0.975\n",
    "            \n",
    "            \n",
    "        inputs.Tsolid.value = (tsol_slope * prediction) + tsol_intercept\n",
    "        inputs.Tliquid.value = (tliq_slope * prediction) + tliq_intercept\n",
    "        r = Run(MeltHEA, inputs)\n",
    "        coexist = bool(r.read('Coexistence'))\n",
    "        conv = bool(r.read('Converged'))\n",
    "        \n",
    "    \n",
    "    melting_temp = float(r.read('melting_temperature'))\n",
    "    \n",
    "    prediction_row = prediction_row.drop('Predicted Melting Temp', axis = 1)\n",
    "    prediction_row['output.melting_temperature'] = melting_temp\n",
    "    known = pd.concat([known, prediction_row], ignore_index=True)\n",
    "    \n",
    "    new_row = {\n",
    "        'exp': i,\n",
    "        'Cr': float(prediction_row['Cr']),\n",
    "        'Co': float(prediction_row['Co']),\n",
    "        'Cu': float(prediction_row['Cu']),\n",
    "        'Fe': float(prediction_row['Fe']),\n",
    "        'Ni': float(prediction_row['Ni']),\n",
    "        'melting_temp': melting_temp,\n",
    "        'ci': r.read('melting_temperature_ci'),\n",
    "        'count' : counter\n",
    "    }\n",
    "    \n",
    "    list_df = list_df.append(new_row, ignore_index=True)\n",
    "    name = 'active_learning_' + iaf + '.csv'\n",
    "    list_df.to_csv(name, index=False)\n",
    "    \n",
    "    counter_list.append(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80043f0",
   "metadata": {},
   "source": [
    "# Active Learning Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4aa8ab",
   "metadata": {},
   "source": [
    "Plot to evaluate and compare active learning using FAIR data principles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5040455b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T00:15:04.656279Z",
     "start_time": "2024-07-23T00:15:04.220Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2433c5ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-23T00:15:04.657387Z",
     "start_time": "2024-07-23T00:15:04.221Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# FORMATS\n",
    "colors = ['teal', 'brown', 'goldenrod', 'purple', 'forestgreen']\n",
    "plt.rcParams[\"font.size\"] = 20\n",
    "\n",
    "# VALUES\n",
    "\n",
    "main_values = df.iloc[:, 6]  # 7th column for melting temperature\n",
    "confidence_interval = df.iloc[:, 7]  # 8th column for confidence intervals\n",
    "\n",
    "composition_columns = df.iloc[:, 1:6]  # Columns 2 to 6 for compositions\n",
    "composition_normalized = composition_columns.div(composition_columns.sum(axis=1), axis=0)\n",
    "composition_normalized = composition_columns.apply(lambda x: x * 100)\n",
    "\n",
    "# Plot setup\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "\n",
    "ax1.plot(df.index + 1, main_values, marker='o', linestyle='-', color='black', zorder=10, label='MEI Acquisition Function')\n",
    "ax1.set_ylabel('Melting Temperature (K)')\n",
    "#ax1.legend(loc=(1.15, 0.90), fontsize=14)\n",
    "ax1.set_xlabel('AL Iterations ({} Acquisition)'.format(iaf))\n",
    "ax1.set_xlim([0,16])\n",
    "ax1.set_facecolor('none')\n",
    "\n",
    "ax2 = fig.add_axes(ax1.get_position(), frame_on=False, zorder=-1)  # zorder set to -1 to be behind ax1\n",
    "ax2.patch.set_visible(False)  # Hide the 'ax2' patch to allow 'ax1' to be visible\n",
    "ax2.yaxis.tick_right()\n",
    "ax2.yaxis.set_label_position('right')\n",
    "\n",
    "\n",
    "\n",
    "# Plotting each composition as a stacked bar\n",
    "for i, col in enumerate(composition_normalized.columns):\n",
    "    bars = ax2.bar(df.index + 1, composition_normalized.iloc[:, i],\n",
    "                   bottom=composition_normalized.iloc[:, :i].sum(axis=1) if i > 0 else 0,\n",
    "                   label=col, alpha=0.5, color=colors[i], zorder=1)\n",
    "    # Adding text labels to each bar only if bar is visible\n",
    "    for rect in bars:\n",
    "        height = rect.get_height()\n",
    "        if height > 0:  # Only add text if the bar is visible\n",
    "            ax2.text(rect.get_x() + rect.get_width() / 2, rect.get_y() + height / 2,\n",
    "                     col, ha='center', va='center', color='white', fontweight='bold', fontsize=16)\n",
    "\n",
    "# Axes labels and grid configuration\n",
    "\n",
    "ax2.set_ylabel('Composition (%)', color='gray')\n",
    "ax2.tick_params(size=12, axis='y', colors='gray')\n",
    "ax2.grid(True, which='major', axis='y', linestyle='-', linewidth='0.25', color='gray')\n",
    "#ax2.legend(loc=(1.15, 0.10), title='Composition', fontsize=14)\n",
    "\n",
    "ax2.set_xlim([0,16])  # Ensure x-axis limits are identical\n",
    "ax2.set_xticks(ax1.get_xticks())  # Align the x-tick positions\n",
    "ax2.set_xticklabels(ax1.get_xticklabels())  # Optionally match tick labels if needed\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
